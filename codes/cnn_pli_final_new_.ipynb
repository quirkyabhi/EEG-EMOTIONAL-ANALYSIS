{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cnn_pli_final_new .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quirkyabhi/EEG-EMOTIONAL-ANALYSIS/blob/master/codes/cnn_pli_final_new_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6q29n2KZTquf",
        "outputId": "1767617a-17ea-40c1-bf26-d4afa5745838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('tf')\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "from scipy.interpolate import griddata\n",
        "from sklearn.preprocessing import scale\n",
        "from functools import reduce\n",
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense, Conv2D, MaxPooling2D, Conv1D, MaxPool1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Input, BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "# from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adadelta\n",
        "# import plotly.graph_objs as go\n",
        "# from matplotlib.pyplot import cm\n",
        "# from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "# import h5py\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGWIuytZUSwA",
        "colab": {}
      },
      "source": [
        "def load(datafile):\n",
        "    \n",
        "    try:\n",
        "        dataMat = scipy.io.loadmat('images_average.mat', mat_dtype=True)\n",
        "        print(\"Data loading complete. Shape is %r\" % (dataMat['images_plv'].shape))\n",
        "    except:\n",
        "        try:\n",
        "            dataMat = pd.read_csv(datafile,  index=False, header= None)\n",
        "        except:\n",
        "            dataMat=pd.read_excel(datafile, index=False, header= None)\n",
        "        \n",
        "    try:\n",
        "        return dataMat['images_plv']\n",
        "    except:\n",
        "        return dataMat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_R3OiXFUZbx",
        "colab": {}
      },
      "source": [
        "def reformatInput(data, labels):\n",
        "    indices = np.random.permutation(147200)\n",
        "\n",
        "    trainIndices = [indices[:int(147200*.8)]]\n",
        "    validIndices = [indices[int(147200*.8):]]\n",
        "\n",
        "    if data.ndim == 3:\n",
        "        return [(data[trainIndices], np.squeeze(labels[trainIndices]).astype(np.int32)),\n",
        "                (data[validIndices], np.squeeze(labels[validIndices]).astype(np.int32))]\n",
        "#                 (data[testIndices], np.squeeze(labels[testIndices]).astype(np.int32))]\n",
        "    elif data.ndim == 5:\n",
        "        return [(data[:, trainIndices], np.squeeze(labels[trainIndices]).astype(np.int32)),\n",
        "                (data[:, validIndices], np.squeeze(labels[validIndices]).astype(np.int32))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fnRFTfuUaBF",
        "colab": {}
      },
      "source": [
        "def make_matrix(df):\n",
        "#     mat=np.array(df[1,:])\n",
        "    return df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDrdYj_Be4a6",
        "colab": {}
      },
      "source": [
        "# df=pd.read_csv('plv_csv.csv', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_OZl15v_Vsjk",
        "outputId": "cda47aaa-890f-48d7-d314-7a529552f990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df=pd.read_csv('drive/My Drive/EEG/PLV_final_dist2.csv',  header=None)\n",
        "ldf=pd.read_csv('drive/My Drive/EEG/arousal_label_total.csv', header= None)\n",
        "mat=make_matrix(df)\n",
        "ldf=make_matrix(ldf)\n",
        "ldf=np.asarray(ldf)\n",
        "mat.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147200, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TH9kQ5vIUgbq",
        "outputId": "a1369be6-805e-488c-9856-2b09f964f89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "finalmat=[]\n",
        "ldf.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147200, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cXW0XGPJUk8P",
        "colab": {}
      },
      "source": [
        "for i in range(len(mat)):\n",
        "    finalmat.append(mat[i,:].reshape(32,32))\n",
        "train=np.asarray(finalmat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7-t_b3jUr4W",
        "outputId": "a0ae4774-38f1-4de2-be44-346d0e26c0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = reformatInput(train, ldf)\n",
        "# y_train=to_categorical(y_train)\n",
        "# y_test=to_categorical(y_test)\n",
        "X_train = X_train.astype(float).reshape(117760,32,32,1)\n",
        "X_test = X_test.astype(float).reshape(147200-117760,32,32,1)\n",
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 1, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3qs_WP-vhhGE",
        "outputId": "30675e1d-0671-4528-fb6a-f8dae3b88074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('tf')\n",
        "def make_model():\n",
        "  \n",
        "  num_category = 2\n",
        "  # t_train=y_train\n",
        "  # y_test=y_val\n",
        "  # y_train = keras.utils.to_categorical(y_train, num_category)\n",
        "  # y_test = keras.utils.to_categorical(y_val, num_category)\n",
        "  model = Sequential()\n",
        "  #convolutional layer with rectified linear unit activation\n",
        "  model.add(Conv2D(32, kernel_size=3,activation='tanh',input_shape=(32,32,1), ))\n",
        "  #32 convolution filters used each of size 3x3\n",
        "  #again\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(BatchNormalization())\n",
        "#   model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, 3, activation=keras.activations.tanh))\n",
        "  model.add(Conv2D(128, kernel_size=3,activation='tanh'))\n",
        "#   #64 convolution filters used each of size 3x3\n",
        "#   #choose the best features via pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(BatchNormalization())\n",
        "#   model.add(Dropout(0.25))\n",
        "#   model.add(Conv2D(128, kernel_size=3,activation='relu'))\n",
        "#   model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#   model.add(BatchNormalization())\n",
        "#   model.add(Dropout(0.2))\n",
        "  # randomly turn neurons on and off to improve convergence\n",
        "  # model.add(Dropout(0.25))\n",
        "  # model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "  # #64 convolution filters used each of size 3x3\n",
        "  # #choose the best features via pooling\n",
        "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Dropout(0.2))\n",
        "  # model.add(Conv2D(512, kernel_size=(3, 3),\n",
        "  #                  activation='relu'))\n",
        "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Dropout(0.1))\n",
        "  # # randomly turn neurons on and off to improve convergence\n",
        "  # model.add(Dropout(0.25))\n",
        "  # # flatten since too many dimensions, we only want a classification output\n",
        "  model.add(Flatten())\n",
        "  #fully connected to get all relevant data\n",
        "  model.add(Dense(128, activation='tanh'))\n",
        "  #one more dropout for convergence' sake :) \n",
        "#   model.add(Dropout(0.5))\n",
        "  #output a softmax to squash the matrix into output probabilities\n",
        "  model.add(Dense(2, activation='softplus'))\n",
        "  print(model.summary())\n",
        "#   model.compile(loss=keras.losses.binary_crossentropy,\n",
        "#               optimizer=keras.optimizers.Adam(.0001),\n",
        "#               metrics=['accuracy'])\n",
        "  return model\n",
        "model=make_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0720 08:37:02.800003 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0720 08:37:02.841671 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0720 08:37:02.850597 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0720 08:37:02.866967 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0720 08:37:02.883432 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0720 08:37:02.884457 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0720 08:37:05.054636 139810037974912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 15, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               409728    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 503,298\n",
            "Trainable params: 502,978\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjotFSg_f8Ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(y_train.shape)\n",
        "# model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "#               optimizer=keras.optimizers.Adadelta(),\n",
        "#               metrics=['accuracy'])\n",
        "# batch_size = 256\n",
        "# num_epoch = 100\n",
        "# #model training\n",
        "# model_log = model.fit(X_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=num_epoch,\n",
        "#           verbose=1,\n",
        "#           validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3DmYWI00iH8x",
        "outputId": "482e3517-5d7f-4a53-ac4f-8c133d4a25e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(y_train.shape)\n",
        "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.SGD(.00001, decay=10**-6, momentum=0.9, nesterov=True),\n",
        "              metrics=['accuracy'])\n",
        "batch_size = 256\n",
        "num_epoch = 1000\n",
        "#model training\n",
        "model_log = model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epoch,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117760,)\n",
            "Train on 117760 samples, validate on 29440 samples\n",
            "Epoch 1/1000\n",
            "117760/117760 [==============================] - 10s 83us/step - loss: 0.6719 - acc: 0.5982 - val_loss: 0.6756 - val_acc: 0.5910\n",
            "Epoch 2/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6719 - acc: 0.5984 - val_loss: 0.6757 - val_acc: 0.5908\n",
            "Epoch 3/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6718 - acc: 0.5984 - val_loss: 0.6760 - val_acc: 0.5908\n",
            "Epoch 4/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6718 - acc: 0.5984 - val_loss: 0.6756 - val_acc: 0.5913\n",
            "Epoch 5/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6718 - acc: 0.5982 - val_loss: 0.6755 - val_acc: 0.5914\n",
            "Epoch 6/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6718 - acc: 0.5984 - val_loss: 0.6755 - val_acc: 0.5911\n",
            "Epoch 7/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6717 - acc: 0.5985 - val_loss: 0.6757 - val_acc: 0.5907\n",
            "Epoch 8/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6717 - acc: 0.5987 - val_loss: 0.6757 - val_acc: 0.5909\n",
            "Epoch 9/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6717 - acc: 0.5984 - val_loss: 0.6756 - val_acc: 0.5914\n",
            "Epoch 10/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6716 - acc: 0.5984 - val_loss: 0.6755 - val_acc: 0.5911\n",
            "Epoch 11/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6716 - acc: 0.5984 - val_loss: 0.6755 - val_acc: 0.5915\n",
            "Epoch 12/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6716 - acc: 0.5987 - val_loss: 0.6757 - val_acc: 0.5908\n",
            "Epoch 13/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6715 - acc: 0.5987 - val_loss: 0.6754 - val_acc: 0.5910\n",
            "Epoch 14/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6715 - acc: 0.5988 - val_loss: 0.6755 - val_acc: 0.5907\n",
            "Epoch 15/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6715 - acc: 0.5987 - val_loss: 0.6754 - val_acc: 0.5917\n",
            "Epoch 16/1000\n",
            "117760/117760 [==============================] - 9s 77us/step - loss: 0.6715 - acc: 0.5989 - val_loss: 0.6755 - val_acc: 0.5906\n",
            "Epoch 17/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6715 - acc: 0.5986 - val_loss: 0.6755 - val_acc: 0.5917\n",
            "Epoch 18/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6714 - acc: 0.5992 - val_loss: 0.6754 - val_acc: 0.5910\n",
            "Epoch 19/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6714 - acc: 0.5990 - val_loss: 0.6754 - val_acc: 0.5911\n",
            "Epoch 20/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6713 - acc: 0.5990 - val_loss: 0.6754 - val_acc: 0.5914\n",
            "Epoch 21/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6713 - acc: 0.5989 - val_loss: 0.6753 - val_acc: 0.5912\n",
            "Epoch 22/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6713 - acc: 0.5989 - val_loss: 0.6753 - val_acc: 0.5915\n",
            "Epoch 23/1000\n",
            "117760/117760 [==============================] - 9s 81us/step - loss: 0.6712 - acc: 0.5993 - val_loss: 0.6755 - val_acc: 0.5915\n",
            "Epoch 24/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6712 - acc: 0.5994 - val_loss: 0.6753 - val_acc: 0.5913\n",
            "Epoch 25/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6712 - acc: 0.5991 - val_loss: 0.6754 - val_acc: 0.5915\n",
            "Epoch 26/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6712 - acc: 0.5992 - val_loss: 0.6754 - val_acc: 0.5913\n",
            "Epoch 27/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6711 - acc: 0.5994 - val_loss: 0.6752 - val_acc: 0.5911\n",
            "Epoch 28/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6711 - acc: 0.5993 - val_loss: 0.6753 - val_acc: 0.5915\n",
            "Epoch 29/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6711 - acc: 0.5993 - val_loss: 0.6753 - val_acc: 0.5916\n",
            "Epoch 30/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6711 - acc: 0.5992 - val_loss: 0.6754 - val_acc: 0.5917\n",
            "Epoch 31/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6711 - acc: 0.5990 - val_loss: 0.6752 - val_acc: 0.5915\n",
            "Epoch 32/1000\n",
            "117760/117760 [==============================] - 9s 76us/step - loss: 0.6710 - acc: 0.5996 - val_loss: 0.6752 - val_acc: 0.5917\n",
            "Epoch 33/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6710 - acc: 0.5994 - val_loss: 0.6752 - val_acc: 0.5914\n",
            "Epoch 34/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6710 - acc: 0.5996 - val_loss: 0.6753 - val_acc: 0.5915\n",
            "Epoch 35/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6709 - acc: 0.5995 - val_loss: 0.6753 - val_acc: 0.5915\n",
            "Epoch 36/1000\n",
            "117760/117760 [==============================] - 9s 78us/step - loss: 0.6709 - acc: 0.5996 - val_loss: 0.6752 - val_acc: 0.5913\n",
            "Epoch 37/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6709 - acc: 0.5997 - val_loss: 0.6752 - val_acc: 0.5916\n",
            "Epoch 38/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6708 - acc: 0.5998 - val_loss: 0.6752 - val_acc: 0.5913\n",
            "Epoch 39/1000\n",
            "117760/117760 [==============================] - 9s 80us/step - loss: 0.6708 - acc: 0.5997 - val_loss: 0.6753 - val_acc: 0.5914\n",
            "Epoch 40/1000\n",
            "117760/117760 [==============================] - 9s 79us/step - loss: 0.6708 - acc: 0.5992 - val_loss: 0.6752 - val_acc: 0.5915\n",
            "Epoch 41/1000\n",
            " 11264/117760 [=>............................] - ETA: 7s - loss: 0.6711 - acc: 0.6001"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wqjGtGUS43DO",
        "colab": {}
      },
      "source": [
        "# a=(model.predict(X_train))\n",
        "# accuracy_score(y_pred=a,y_true=y_test)\n",
        "# neural_network = KerasClassifier(build_fn=model, \n",
        "#                                  epochs=1000, \n",
        "#                                  batch_size=100, \n",
        "#                                  verbose=0)\n",
        "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "# cross_val_score(model, train, ldf, cv=kfold, scoring=\"accuracy\")\n",
        "# y_test\n",
        "# a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAf6jO87ib_C",
        "colab": {}
      },
      "source": [
        "# def fold(k):\n",
        "#   folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(train, ldf))\n",
        "#   return folds, X_train, y_train\n",
        "\n",
        "# k = 7\n",
        "# folds, X_train, y_train = fold(k=7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2_oKGEPGreab",
        "colab": {}
      },
      "source": [
        "# for j, (train_idx, val_idx) in enumerate(folds):\n",
        "    \n",
        "#     print('\\nFold ',j)\n",
        "#     X_train_cv = X_train[train_idx]\n",
        "#     y_train_cv = y_train[train_idx]\n",
        "#     X_valid_cv = X_train[val_idx]\n",
        "#     y_valid_cv= y_train[val_idx]\n",
        "    \n",
        "# #     name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
        "# #     callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
        "# #     generator = gen.flow(X_train_cv, y_train_cv, batch_size = batch_size)\n",
        "# #     model = get_model()\n",
        "#     model.fit(\n",
        "# #           generator,\n",
        "#           steps_per_epoch=len(X_train_cv)/batch_size,\n",
        "#           epochs=15,\n",
        "#           shuffle=True,\n",
        "#           verbose=1,\n",
        "#           validation_data = (X_valid_cv, y_valid_cv),\n",
        "#           callbacks = callbacks)\n",
        "    \n",
        "#     print(model.evaluate(X_valid_cv, y_valid_cv))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IYi1zKDm2T9m",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}